{
 "cells": [
  {
   "cell_type": "code",
   "id": "1828e84d-c0fa-4d5d-aa62-11f4acbe17d6",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-03T16:20:04.498138Z",
     "start_time": "2024-07-03T16:19:58.346771Z"
    }
   },
   "source": [
    "# Import of libraries\n",
    "import random\n",
    "import imageio\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "from torchvision.datasets.mnist import MNIST, FashionMNIST"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "947a5a4d-37bf-437d-8c70-bf798f22ec5d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-03T16:20:04.544445Z",
     "start_time": "2024-07-03T16:20:04.500060Z"
    }
   },
   "source": [
    "# Setting reproducibility\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Definitions\n",
    "STORE_PATH_MNIST = r\"checkpoints/ddpm_mnist.pt\"\n",
    "STORE_PATH_FASHION = r\"checkpoints/ddpm_fashion.pt\""
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c07b31f6-509f-41c9-a063-e889d99f53b8",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-03T16:20:05.754543Z",
     "start_time": "2024-07-03T16:20:05.743024Z"
    }
   },
   "source": [
    "train_flag = False\n",
    "fashion = False\n",
    "batch_size = 128\n",
    "n_epochs = 40\n",
    "lr = 0.001"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0fc6348a-cc81-4e88-abde-6d6d4c39363a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-03T16:20:06.392411Z",
     "start_time": "2024-07-03T16:20:06.387701Z"
    }
   },
   "source": "store_path = STORE_PATH_FASHION if fashion else STORE_PATH_MNIST",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fa68ac17-b4f1-493a-a9d4-e5d188a92513",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c4ec0aa-6a2e-48f2-810d-3f272b4801aa",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-03T16:20:07.186192Z",
     "start_time": "2024-07-03T16:20:07.170822Z"
    }
   },
   "source": [
    "def show_images(images, title=\"\"):\n",
    "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
    "\n",
    "    # Converting images to CPU numpy arrays\n",
    "    if type(images) is torch.Tensor:\n",
    "        images = images.detach().cpu().numpy()\n",
    "\n",
    "    # Defining number of rows and columns\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    rows = int(len(images) ** (1 / 2))\n",
    "    cols = round(len(images) / rows)\n",
    "\n",
    "    # Populating figure with sub-plots\n",
    "    idx = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            fig.add_subplot(rows, cols, idx + 1)\n",
    "\n",
    "            if idx < len(images):\n",
    "                plt.imshow(images[idx][0], cmap=\"gray\")\n",
    "                idx += 1\n",
    "    fig.suptitle(title, fontsize=30)\n",
    "\n",
    "    # Showing the figure\n",
    "    plt.show()"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c0230010-c2ea-41f5-9734-bd0b51d1a35c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-03T16:20:07.403878Z",
     "start_time": "2024-07-03T16:20:07.395976Z"
    }
   },
   "source": [
    "def show_first_batch(loader):\n",
    "    for batch in loader:\n",
    "        show_images(batch[0], \"Images in the first batch\")\n",
    "        break"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "642b2714-e819-4053-9d1a-5833857210ca",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "id": "4e7108d8-27b7-4f2e-a785-f53eb75d9a46",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-03T16:28:51.172584Z",
     "start_time": "2024-07-03T16:20:07.716713Z"
    }
   },
   "source": [
    "# Loading the data (converting each image into a tensor and normalizing between [-1, 1])\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: (x - 0.5) * 2)]\n",
    ")\n",
    "ds_fn = FashionMNIST if fashion else MNIST\n",
    "dataset = ds_fn(\"./datasets\", download=True, train=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size, shuffle=True)"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "30cd10b6-f4d6-41a5-a027-ac40a9111b15",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-03T16:28:51.175807Z",
     "start_time": "2024-07-03T16:28:51.174797Z"
    }
   },
   "source": [
    "# Optionally, show a batch of regular images\n",
    "show_first_batch(loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6b508a2b-fb9e-4f9e-a4b2-30db86513461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T16:28:51.176822Z",
     "start_time": "2024-07-03T16:28:51.176822Z"
    }
   },
   "source": [
    "# Getting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\t\" + (f\"{torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e5cf72b1-34f7-4213-919b-491f87bd3099",
   "metadata": {},
   "source": [
    "# Defining the DDPM module"
   ]
  },
  {
   "cell_type": "code",
   "id": "5700377a-e3f0-4b0b-90ed-3b0f9499e4f6",
   "metadata": {},
   "source": [
    "# DDPM class\n",
    "class MyDDPM(nn.Module):\n",
    "    def __init__(self, network, n_steps=200, min_beta=10 ** -4, max_beta=0.02, device=None, image_chw=(1, 28, 28)):\n",
    "        super(MyDDPM, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "        self.image_chw = image_chw\n",
    "        self.network = network.to(device)\n",
    "        self.betas = torch.linspace(min_beta, max_beta, n_steps).to(\n",
    "            device)  # Number of steps is typically in the order of thousands\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))]).to(device)\n",
    "\n",
    "    def forward(self, x0, t, eta=None):\n",
    "        # Make input image more noisy (we can directly skip to the desired step)\n",
    "        n, c, h, w = x0.shape\n",
    "        a_bar = self.alpha_bars[t]\n",
    "\n",
    "        if eta is None:\n",
    "            eta = torch.randn(n, c, h, w).to(self.device)\n",
    "\n",
    "        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * eta\n",
    "        return noisy\n",
    "\n",
    "    def backward(self, x, t):\n",
    "        # Run each image through the network for each timestep t in the vector t.\n",
    "        # The network returns its estimation of the noise that was added.\n",
    "        return self.network(x, t)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9db4ee02-798b-40a3-bc52-95969a640ae4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualizing forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "id": "992ea6b9-f0da-4ea1-992b-7cef321bb289",
   "metadata": {
    "tags": []
   },
   "source": [
    "def show_forward(ddpm, loader, device):\n",
    "    # Showing the forward process\n",
    "    for batch in loader:\n",
    "        imgs = batch[0]\n",
    "\n",
    "        show_images(imgs, \"Original images\")\n",
    "\n",
    "        for percent in [0.25, 0.5, 0.75, 1]:\n",
    "            show_images(\n",
    "                ddpm(imgs.to(device),\n",
    "                     [int(percent * ddpm.n_steps) - 1 for _ in range(len(imgs))]),\n",
    "                f\"DDPM Noisy images {int(percent * 100)}%\"\n",
    "            )\n",
    "        break"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![](assets/ddpm.png)\n",
    "$$\n",
    "DDIM: x_{t-1} = \\sqrt{\\bar{\\alpha_{t-1}}}\\hat{x_{0|t}}+\\sqrt{1-\\bar{\\alpha_{t-1}}-\\sigma_t^2}\\epsilon_{\\theta}(x_t,t)+\\sigma_t\\epsilon\n",
    "$$\n",
    "[扩散模型 Diffusion Model 2-3 DDIM](https://www.bilibili.com/video/BV13P411J7dm)"
   ],
   "id": "30e04f52c40db6a0"
  },
  {
   "cell_type": "code",
   "id": "ef2b51e5-d193-4424-a338-bb12ea444458",
   "metadata": {},
   "source": [
    "def generate_new_images(ddpm, n_samples=16, device=None, frames_per_gif=100, gif_name=\"sampling.gif\", c=1, h=28, w=28):\n",
    "    \"\"\"Given a DDPM model, a number of samples to be generated and a device, returns some newly generated samples\"\"\"\n",
    "    frame_idxs = np.linspace(0, ddpm.n_steps, frames_per_gif).astype(np.uint)\n",
    "    frames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            device = ddpm.device\n",
    "\n",
    "        # Starting from random noise\n",
    "        x = torch.randn(n_samples, c, h, w).to(device)\n",
    "\n",
    "        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):\n",
    "            # Estimating noise to be removed\n",
    "            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()\n",
    "            eta_theta = ddpm.backward(x, time_tensor)\n",
    "\n",
    "            alpha_t = ddpm.alphas[t]\n",
    "            alpha_t_bar = ddpm.alpha_bars[t]\n",
    "\n",
    "            # Partially denoising the image\n",
    "            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)\n",
    "\n",
    "            if t > 0:\n",
    "                z = torch.randn(n_samples, c, h, w).to(device)\n",
    "\n",
    "                # Option 1: sigma_t squared = beta_t\n",
    "                beta_t = ddpm.betas[t]\n",
    "                sigma_t = beta_t.sqrt()\n",
    "\n",
    "                # Option 2: sigma_t squared = beta_tilda_t\n",
    "                # prev_alpha_t_bar = ddpm.alpha_bars[t-1] if t > 0 else ddpm.alphas[0]\n",
    "                # beta_tilda_t = ((1 - prev_alpha_t_bar)/(1 - alpha_t_bar)) * beta_t\n",
    "                # sigma_t = beta_tilda_t.sqrt()\n",
    "\n",
    "                # Adding some more noise like in Langevin Dynamics fashion\n",
    "                x = x + sigma_t * z\n",
    "                # 另一种策略，由于eta_theta也是采样于N(0,1)，所以也可以用预测的噪声\n",
    "                # x = x + sigma_t * eta_theta\n",
    "                \n",
    "\n",
    "            # Adding frames to the GIF\n",
    "            if idx in frame_idxs or t == 0:\n",
    "                # Putting digits in range [0, 255]\n",
    "                normalized = x.clone()\n",
    "                for i in range(len(normalized)):\n",
    "                    normalized[i] -= torch.min(normalized[i])\n",
    "                    normalized[i] *= 255 / torch.max(normalized[i])\n",
    "\n",
    "                # Reshaping batch (n, c, h, w) to be a (as much as it gets) square frame\n",
    "                frame = einops.rearrange(normalized, \"(b1 b2) c h w -> (b1 h) (b2 w) c\", b1=int(n_samples ** 0.5))\n",
    "                frame = frame.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "                # Rendering frame\n",
    "                frames.append(frame)\n",
    "\n",
    "    # Storing the gif\n",
    "    with imageio.get_writer(gif_name, mode=\"I\") as writer:\n",
    "        for idx, frame in enumerate(frames):\n",
    "            # Convert grayscale frame to RGB\n",
    "            rgb_frame = np.repeat(frame, 3, axis=-1)\n",
    "            writer.append_data(rgb_frame)\n",
    "            if idx == len(frames) - 1:\n",
    "                for _ in range(frames_per_gif // 3):\n",
    "                    writer.append_data(rgb_frame)\n",
    "    return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "74792648-e347-4faa-982c-8dddcb0d9cf5",
   "metadata": {},
   "source": [
    "# UNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "id": "a3f09537-8276-4b23-a4e7-25378d964f72",
   "metadata": {},
   "source": [
    "def sinusoidal_embedding(n, d):\n",
    "    # Returns the standard positional embedding\n",
    "    embedding = torch.zeros(n, d)\n",
    "    wk = torch.tensor([1 / 10000 ** (2 * j / d) for j in range(d)])\n",
    "    wk = wk.reshape((1, d))\n",
    "    t = torch.arange(n).reshape((n, 1))\n",
    "    embedding[:,::2] = torch.sin(t * wk[:,::2])\n",
    "    embedding[:,1::2] = torch.cos(t * wk[:,::2])\n",
    "\n",
    "    return embedding"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "759b00c4-8f2b-46a8-afda-85d191478858",
   "metadata": {},
   "source": [
    "class MyBlock(nn.Module):\n",
    "    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):\n",
    "        super(MyBlock, self).__init__()\n",
    "        self.ln = nn.LayerNorm(shape)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)\n",
    "        self.activation = nn.SiLU() if activation is None else activation\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ln(x) if self.normalize else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.activation(out)\n",
    "        return out"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2179e701-9b13-4a92-be9c-bbcddcb100f1",
   "metadata": {},
   "source": [
    "class MyUNet(nn.Module):\n",
    "    def __init__(self, n_steps=1000, time_emb_dim=100):\n",
    "        super(MyUNet, self).__init__()\n",
    "\n",
    "        # Sinusoidal embedding\n",
    "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.requires_grad_(False)\n",
    "\n",
    "        # First half\n",
    "        self.te1 = self._make_te(time_emb_dim, 1)\n",
    "        self.b1 = nn.Sequential(\n",
    "            MyBlock((1, 28, 28), 1, 10),\n",
    "            MyBlock((10, 28, 28), 10, 10),\n",
    "            MyBlock((10, 28, 28), 10, 10)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)\n",
    "\n",
    "        self.te2 = self._make_te(time_emb_dim, 10)\n",
    "        self.b2 = nn.Sequential(\n",
    "            MyBlock((10, 14, 14), 10, 20),\n",
    "            MyBlock((20, 14, 14), 20, 20),\n",
    "            MyBlock((20, 14, 14), 20, 20)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
    "\n",
    "        self.te3 = self._make_te(time_emb_dim, 20)\n",
    "        self.b3 = nn.Sequential(\n",
    "            MyBlock((20, 7, 7), 20, 40),\n",
    "            MyBlock((40, 7, 7), 40, 40),\n",
    "            MyBlock((40, 7, 7), 40, 40)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(40, 40, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(40, 40, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.te_mid = self._make_te(time_emb_dim, 40)\n",
    "        self.b_mid = nn.Sequential(\n",
    "            MyBlock((40, 3, 3), 40, 20),\n",
    "            MyBlock((20, 3, 3), 20, 20),\n",
    "            MyBlock((20, 3, 3), 20, 40)\n",
    "        )\n",
    "\n",
    "        # Second half\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(40, 40, 4, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.ConvTranspose2d(40, 40, 2, 1)\n",
    "        )\n",
    "\n",
    "        self.te4 = self._make_te(time_emb_dim, 80)\n",
    "        self.b4 = nn.Sequential(\n",
    "            MyBlock((80, 7, 7), 80, 40),\n",
    "            MyBlock((40, 7, 7), 40, 20),\n",
    "            MyBlock((20, 7, 7), 20, 20)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
    "        self.te5 = self._make_te(time_emb_dim, 40)\n",
    "        self.b5 = nn.Sequential(\n",
    "            MyBlock((40, 14, 14), 40, 20),\n",
    "            MyBlock((20, 14, 14), 20, 10),\n",
    "            MyBlock((10, 14, 14), 10, 10)\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)\n",
    "        self.te_out = self._make_te(time_emb_dim, 20)\n",
    "        self.b_out = nn.Sequential(\n",
    "            MyBlock((20, 28, 28), 20, 10),\n",
    "            MyBlock((10, 28, 28), 10, 10),\n",
    "            MyBlock((10, 28, 28), 10, 10, normalize=False)\n",
    "        )\n",
    "\n",
    "        self.conv_out = nn.Conv2d(10, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # x is (N, 2, 28, 28) (image with positional embedding stacked on channel dimension)\n",
    "        t = self.time_embed(t)\n",
    "        n = len(x)\n",
    "        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (N, 10, 28, 28)\n",
    "        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (N, 20, 14, 14)\n",
    "        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (N, 40, 7, 7)\n",
    "\n",
    "        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 40, 3, 3)\n",
    "\n",
    "        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (N, 80, 7, 7)\n",
    "        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (N, 20, 7, 7)\n",
    "\n",
    "        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (N, 40, 14, 14)\n",
    "        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (N, 10, 14, 14)\n",
    "\n",
    "        out = torch.cat((out1, self.up3(out5)), dim=1)  # (N, 20, 28, 28)\n",
    "        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (N, 1, 28, 28)\n",
    "\n",
    "        out = self.conv_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_te(self, dim_in, dim_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(dim_in, dim_out),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_out, dim_out)\n",
    "        )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fd9a7043-8d8d-47e9-80e8-9a023707aab0",
   "metadata": {},
   "source": [
    "# Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "26071cae-a246-4800-b30b-cb7d3c468f0f",
   "metadata": {},
   "source": [
    "# Defining model\n",
    "n_steps, min_beta, max_beta = 1000, 10 ** -4, 0.02  # Originally used by the authors\n",
    "ddpm = MyDDPM(MyUNet(n_steps), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "db72ebed-0af8-4370-9570-99ad188761bb",
   "metadata": {},
   "source": [
    "sum([p.numel() for p in ddpm.parameters()])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c269976e-309d-4069-b666-e467dd5ce15a",
   "metadata": {},
   "source": [
    "# Optional visualizations"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0f63972-b342-4c63-b968-428da26f59f6",
   "metadata": {},
   "source": [
    "# load pretrained model\n",
    "print(store_path)\n",
    "# Optionally, load a pre-trained model that will be further trained\n",
    "# ddpm.load_state_dict(torch.load(store_path, map_location=device))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "7848480c-edb0-48bc-8e7c-60f6e62cb083",
   "metadata": {},
   "source": [
    "# Optionally, show the diffusion (forward) process\n",
    "show_forward(ddpm, loader, device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ba51cbc0-7296-4864-8cd1-9c5a4d9f7e2d",
   "metadata": {},
   "source": [
    "# Optionally, show the denoising (backward) process\n",
    "generated = generate_new_images(ddpm, gif_name=\"logs/before_training.gif\")\n",
    "show_images(generated, \"Images generated before training\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5e6de2ff-1d72-457d-a685-ce730cd0b891",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "0a1bfb1b-b34f-4e6a-9037-a4be2d35fb15",
   "metadata": {},
   "source": [
    "def training_loop(ddpm, loader, n_epochs, optim, device, display=False, store_path=\"checkpoints/ddpm_model.pt\"):\n",
    "    mse = nn.MSELoss()\n",
    "    best_loss = float(\"inf\")\n",
    "    n_steps = ddpm.n_steps\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs), desc=f\"Training progress\", colour=\"#00ff00\"):\n",
    "        epoch_loss = 0.0\n",
    "        for step, batch in enumerate(tqdm(loader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\", colour=\"#005500\")):\n",
    "            # Loading data\n",
    "            x0 = batch[0].to(device)\n",
    "            n = len(x0)\n",
    "\n",
    "            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars\n",
    "            eta = torch.randn_like(x0).to(device)\n",
    "            t = torch.randint(0, n_steps, (n,)).to(device)\n",
    "\n",
    "            # Computing the noisy image based on x0 and the time-step (forward process)\n",
    "            noisy_imgs = ddpm(x0, t, eta)\n",
    "\n",
    "            # Getting model estimation of noise based on the images and the time-step\n",
    "            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n, -1))\n",
    "\n",
    "            # Optimizing the MSE between the noise plugged and the predicted noise\n",
    "            loss = mse(eta_theta, eta)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            epoch_loss += loss.item() * len(x0) / len(loader.dataset)\n",
    "\n",
    "        # Display images generated at this epoch\n",
    "        if display:\n",
    "            show_images(generate_new_images(ddpm, device=device), f\"Images generated at epoch {epoch + 1}\")\n",
    "\n",
    "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
    "\n",
    "        # Storing the model\n",
    "        if best_loss > epoch_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(ddpm.state_dict(), store_path)\n",
    "            log_string += \" --> Best model ever (stored)\"\n",
    "\n",
    "        print(log_string)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "497e440d-e507-409d-8c10-9b7cebcb5dfa",
   "metadata": {},
   "source": [
    "# Training\n",
    "if train_flag:\n",
    "    training_loop(ddpm, loader, n_epochs, optim=Adam(ddpm.parameters(), lr), device=device, store_path=store_path)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fabca557-ec7c-4839-9b29-703051e2677b",
   "metadata": {},
   "source": [
    "# Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "id": "08eccb65-ca01-4936-b9cc-eb0eefd1ea88",
   "metadata": {},
   "source": [
    "# Loading the trained model\n",
    "best_model = MyDDPM(MyUNet(), n_steps=n_steps, device=device)\n",
    "best_model.load_state_dict(torch.load(store_path, map_location=device))\n",
    "best_model.eval()\n",
    "print(\"Model loaded\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c06777dc-73a0-4288-a499-a44a3e746e30",
   "metadata": {},
   "source": [
    "print(\"Generating new images\")\n",
    "generated = generate_new_images(\n",
    "        best_model,\n",
    "        n_samples=16,\n",
    "        device=device,\n",
    "        gif_name=\"logs/fashion.gif\" if fashion else \"logs/mnist.gif\"\n",
    "    )\n",
    "show_images(generated, \"Final result\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2fae6491-0bb3-48a1-92a2-302fff891258",
   "metadata": {
    "tags": []
   },
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(open('logs/fashion.gif' if fashion else 'logs/mnist.gif','rb').read())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "4f3146fcc5061913",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
